[quyuan@localhost code]$ python main.py
Using TensorFlow backend.
2018-01-09 02:52:09.957360: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 02:52:09.957443: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 02:52:09.957478: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 02:52:09.957526: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 02:52:09.957596: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 02:52:11.344801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:06:00.0
Total memory: 11.92GiB
Free memory: 1.03GiB
2018-01-09 02:52:11.344912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-01-09 02:52:11.344931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-01-09 02:52:11.344955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)
loading data...
train_data shape:  (1104, 1, 16, 16)
min: 0.0 max: 262.0
XC shape:  (1437, 3, 16, 16) Y shape: (1437, 1, 16, 16)
XC shape:  (1437, 3, 16, 16) Y shape: (1437, 1, 16, 16)
1 1
==========
compiling model...
/home/quyuan/GP/code/mydeepst/models/STResNet.py:144: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding="same", filters=64, kernel_size=(3, 3))`
  nb_filter=64, nb_row=3, nb_col=3, border_mode="same")(input)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding="same", strides=(1, 1), filters=64, kernel_size=(3, 3))`
  border_mode="same")(activation)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:22: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  return merge([input, residual], mode='sum')
/usr/lib64/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  name=name)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:151: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding="same", filters=1, kernel_size=(3, 3))`
  nb_filter=1, nb_row=3, nb_col=3, border_mode="same")(activation)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:179: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("ac..., inputs=[<tf.Tenso...)`
  model = Model(input=main_inputs, output=main_output)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 3, 16, 16)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 64, 16, 16)    1792        input_1[0][0]                    
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 64, 16, 16)    0           conv2d_1[0][0]                   
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 64, 16, 16)    36928       activation_1[0][0]               
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 64, 16, 16)    0           conv2d_2[0][0]                   
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 64, 16, 16)    36928       activation_2[0][0]               
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 64, 16, 16)    0           conv2d_1[0][0]                   
                                                                   conv2d_3[0][0]                   
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 64, 16, 16)    0           merge_1[0][0]                    
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 64, 16, 16)    36928       activation_3[0][0]               
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 64, 16, 16)    0           conv2d_4[0][0]                   
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 64, 16, 16)    36928       activation_4[0][0]               
____________________________________________________________________________________________________
merge_2 (Merge)                  (None, 64, 16, 16)    0           merge_1[0][0]                    
                                                                   conv2d_5[0][0]                   
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 64, 16, 16)    0           merge_2[0][0]                    
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 64, 16, 16)    36928       activation_5[0][0]               
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 64, 16, 16)    0           conv2d_6[0][0]                   
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 64, 16, 16)    36928       activation_6[0][0]               
____________________________________________________________________________________________________
merge_3 (Merge)                  (None, 64, 16, 16)    0           merge_2[0][0]                    
                                                                   conv2d_7[0][0]                   
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 64, 16, 16)    0           merge_3[0][0]                    
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 64, 16, 16)    36928       activation_7[0][0]               
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 64, 16, 16)    0           conv2d_8[0][0]                   
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 64, 16, 16)    36928       activation_8[0][0]               
____________________________________________________________________________________________________
merge_4 (Merge)                  (None, 64, 16, 16)    0           merge_3[0][0]                    
                                                                   conv2d_9[0][0]                   
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 64, 16, 16)    0           merge_4[0][0]                    
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 1, 16, 16)     577         activation_9[0][0]               
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 1, 16, 16)     0           conv2d_10[0][0]                  
====================================================================================================
Total params: 297,793
Trainable params: 297,793
Non-trainable params: 0
____________________________________________________________________________________________________
==========
training model...
main.py:202: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  verbose=1)
Train on 990 samples, validate on 111 samples
Epoch 1/500
2018-01-09 02:52:15.536474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)
990/990 [==============================] - 9s - loss: 0.0429 - rmse: 0.1681 - val_loss: 0.0135 - val_rmse: 0.1119
Epoch 2/500
990/990 [==============================] - 5s - loss: 0.0061 - rmse: 0.0773 - val_loss: 0.0043 - val_rmse: 0.0653
Epoch 3/500
990/990 [==============================] - 5s - loss: 0.0037 - rmse: 0.0604 - val_loss: 0.0032 - val_rmse: 0.0563
Epoch 4/500
990/990 [==============================] - 5s - loss: 0.0031 - rmse: 0.0554 - val_loss: 0.0029 - val_rmse: 0.0534
Epoch 5/500
990/990 [==============================] - 5s - loss: 0.0028 - rmse: 0.0525 - val_loss: 0.0027 - val_rmse: 0.0520
Epoch 6/500
990/990 [==============================] - 5s - loss: 0.0026 - rmse: 0.0512 - val_loss: 0.0025 - val_rmse: 0.0497
Epoch 7/500
990/990 [==============================] - 5s - loss: 0.0024 - rmse: 0.0487 - val_loss: 0.0024 - val_rmse: 0.0483
Epoch 8/500
990/990 [==============================] - 5s - loss: 0.0023 - rmse: 0.0479 - val_loss: 0.0026 - val_rmse: 0.0510
Epoch 9/500
990/990 [==============================] - 5s - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0023 - val_rmse: 0.0475
Epoch 10/500
990/990 [==============================] - 5s - loss: 0.0022 - rmse: 0.0468 - val_loss: 0.0036 - val_rmse: 0.0598
Epoch 11/500
990/990 [==============================] - 5s - loss: 0.0022 - rmse: 0.0465 - val_loss: 0.0022 - val_rmse: 0.0462
Epoch 12/500
990/990 [==============================] - 5s - loss: 0.0021 - rmse: 0.0451 - val_loss: 0.0027 - val_rmse: 0.0515
Epoch 13/500
990/990 [==============================] - 5s - loss: 0.0020 - rmse: 0.0447 - val_loss: 0.0023 - val_rmse: 0.0476
Epoch 14/500
990/990 [==============================] - 5s - loss: 0.0021 - rmse: 0.0456 - val_loss: 0.0020 - val_rmse: 0.0448
Epoch 15/500
990/990 [==============================] - 4s - loss: 0.0020 - rmse: 0.0442 - val_loss: 0.0021 - val_rmse: 0.0452
Epoch 16/500
990/990 [==============================] - 5s - loss: 0.0020 - rmse: 0.0442 - val_loss: 0.0020 - val_rmse: 0.0445
Epoch 17/500
990/990 [==============================] - 4s - loss: 0.0021 - rmse: 0.0452 - val_loss: 0.0021 - val_rmse: 0.0455
Epoch 18/500
990/990 [==============================] - 4s - loss: 0.0020 - rmse: 0.0443 - val_loss: 0.0020 - val_rmse: 0.0441
Epoch 19/500
990/990 [==============================] - 5s - loss: 0.0019 - rmse: 0.0431 - val_loss: 0.0019 - val_rmse: 0.0434
Epoch 20/500
990/990 [==============================] - 5s - loss: 0.0018 - rmse: 0.0426 - val_loss: 0.0019 - val_rmse: 0.0436
Epoch 21/500
990/990 [==============================] - 5s - loss: 0.0018 - rmse: 0.0423 - val_loss: 0.0019 - val_rmse: 0.0427
Epoch 22/500
990/990 [==============================] - 5s - loss: 0.0018 - rmse: 0.0422 - val_loss: 0.0022 - val_rmse: 0.0467
Epoch 23/500
990/990 [==============================] - 4s - loss: 0.0018 - rmse: 0.0419 - val_loss: 0.0018 - val_rmse: 0.0425
Epoch 24/500
990/990 [==============================] - 3s - loss: 0.0017 - rmse: 0.0411 - val_loss: 0.0018 - val_rmse: 0.0424
Epoch 25/500
990/990 [==============================] - 3s - loss: 0.0017 - rmse: 0.0416 - val_loss: 0.0019 - val_rmse: 0.0436
Epoch 26/500
990/990 [==============================] - 3s - loss: 0.0018 - rmse: 0.0425 - val_loss: 0.0019 - val_rmse: 0.0437
Epoch 27/500
990/990 [==============================] - 3s - loss: 0.0017 - rmse: 0.0413 - val_loss: 0.0018 - val_rmse: 0.0421
Epoch 28/500
990/990 [==============================] - 6s - loss: 0.0017 - rmse: 0.0413 - val_loss: 0.0018 - val_rmse: 0.0422
Epoch 29/500
990/990 [==============================] - 9s - loss: 0.0018 - rmse: 0.0420 - val_loss: 0.0018 - val_rmse: 0.0417
Epoch 30/500
990/990 [==============================] - 9s - loss: 0.0017 - rmse: 0.0405 - val_loss: 0.0017 - val_rmse: 0.0412
Epoch 31/500
990/990 [==============================] - 9s - loss: 0.0017 - rmse: 0.0412 - val_loss: 0.0024 - val_rmse: 0.0480
Epoch 32/500
990/990 [==============================] - 9s - loss: 0.0018 - rmse: 0.0418 - val_loss: 0.0017 - val_rmse: 0.0411
Epoch 33/500
990/990 [==============================] - 9s - loss: 0.0017 - rmse: 0.0406 - val_loss: 0.0018 - val_rmse: 0.0419
Epoch 34/500
990/990 [==============================] - 9s - loss: 0.0017 - rmse: 0.0405 - val_loss: 0.0018 - val_rmse: 0.0422
Epoch 35/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0402 - val_loss: 0.0017 - val_rmse: 0.0411
Epoch 36/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0402 - val_loss: 0.0018 - val_rmse: 0.0417
Epoch 37/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0396 - val_loss: 0.0018 - val_rmse: 0.0420
Epoch 38/500
990/990 [==============================] - 9s - loss: 0.0018 - rmse: 0.0425 - val_loss: 0.0017 - val_rmse: 0.0405
Epoch 39/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0394 - val_loss: 0.0017 - val_rmse: 0.0412
Epoch 40/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0401 - val_loss: 0.0017 - val_rmse: 0.0405
Epoch 41/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0399 - val_loss: 0.0017 - val_rmse: 0.0405
Epoch 42/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0397 - val_loss: 0.0017 - val_rmse: 0.0409
Epoch 43/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0401 - val_loss: 0.0016 - val_rmse: 0.0400
Epoch 44/500
990/990 [==============================] - 9s - loss: 0.0015 - rmse: 0.0390 - val_loss: 0.0016 - val_rmse: 0.0398
Epoch 45/500
990/990 [==============================] - 9s - loss: 0.0015 - rmse: 0.0388 - val_loss: 0.0016 - val_rmse: 0.0398
Epoch 46/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0394 - val_loss: 0.0018 - val_rmse: 0.0421
Epoch 47/500
990/990 [==============================] - 9s - loss: 0.0017 - rmse: 0.0408 - val_loss: 0.0017 - val_rmse: 0.0410
Epoch 48/500
990/990 [==============================] - 9s - loss: 0.0015 - rmse: 0.0386 - val_loss: 0.0016 - val_rmse: 0.0397
Epoch 49/500
990/990 [==============================] - 9s - loss: 0.0015 - rmse: 0.0387 - val_loss: 0.0016 - val_rmse: 0.0396
Epoch 50/500
990/990 [==============================] - 9s - loss: 0.0016 - rmse: 0.0392 - val_loss: 0.0017 - val_rmse: 0.0404
Epoch 51/500
990/990 [==============================] - 8s - loss: 0.0015 - rmse: 0.0385 - val_loss: 0.0017 - val_rmse: 0.0409
Epoch 52/500
990/990 [==============================] - 7s - loss: 0.0015 - rmse: 0.0386 - val_loss: 0.0016 - val_rmse: 0.0394
Epoch 53/500
990/990 [==============================] - 5s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0016 - val_rmse: 0.0390
Epoch 54/500
990/990 [==============================] - 5s - loss: 0.0015 - rmse: 0.0384 - val_loss: 0.0016 - val_rmse: 0.0397
Epoch 55/500
990/990 [==============================] - 4s - loss: 0.0014 - rmse: 0.0379 - val_loss: 0.0017 - val_rmse: 0.0406
Epoch 56/500
990/990 [==============================] - 4s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0017 - val_rmse: 0.0402
Epoch 57/500
990/990 [==============================] - 4s - loss: 0.0015 - rmse: 0.0386 - val_loss: 0.0016 - val_rmse: 0.0397
Epoch 58/500
990/990 [==============================] - 5s - loss: 0.0015 - rmse: 0.0379 - val_loss: 0.0016 - val_rmse: 0.0395
Epoch 59/500
990/990 [==============================] - 5s - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0017 - val_rmse: 0.0409
==========
evaluating using the model that has the best loss on the valid set
Train score: 0.001545 rmse (norm): 0.038673 rmse (real): 5.066206
Test score: 0.001936 rmse (norm): 0.043996 rmse (real): 5.763519
==========
training model (cont)...
main.py:224: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model_checkpoint], validation_data=(X_test, Y_test))
Train on 1101 samples, validate on 336 samples
Epoch 1/100
1101/1101 [==============================] - 6s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0018 - val_rmse: 0.0412
Epoch 2/100
1101/1101 [==============================] - 6s - loss: 0.0015 - rmse: 0.0383 - val_loss: 0.0018 - val_rmse: 0.0418
Epoch 3/100
1101/1101 [==============================] - 6s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0017 - val_rmse: 0.0399
Epoch 4/100
1101/1101 [==============================] - 6s - loss: 0.0014 - rmse: 0.0377 - val_loss: 0.0017 - val_rmse: 0.0399
Epoch 5/100
1101/1101 [==============================] - 6s - loss: 0.0014 - rmse: 0.0370 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 6/100
1101/1101 [==============================] - 6s - loss: 0.0014 - rmse: 0.0373 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 7/100
1101/1101 [==============================] - 6s - loss: 0.0014 - rmse: 0.0374 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 8/100
1101/1101 [==============================] - 6s - loss: 0.0015 - rmse: 0.0389 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 9/100
1101/1101 [==============================] - 6s - loss: 0.0015 - rmse: 0.0381 - val_loss: 0.0019 - val_rmse: 0.0421
Epoch 10/100
1101/1101 [==============================] - 6s - loss: 0.0014 - rmse: 0.0372 - val_loss: 0.0017 - val_rmse: 0.0404
Epoch 11/100
1101/1101 [==============================] - 6s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0018 - val_rmse: 0.0409
Epoch 12/100
1101/1101 [==============================] - 6s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0016 - val_rmse: 0.0390
Epoch 13/100
1101/1101 [==============================] - 4s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0018 - val_rmse: 0.0413
Epoch 14/100
1101/1101 [==============================] - 4s - loss: 0.0014 - rmse: 0.0372 - val_loss: 0.0017 - val_rmse: 0.0399
Epoch 15/100
1101/1101 [==============================] - 4s - loss: 0.0014 - rmse: 0.0371 - val_loss: 0.0017 - val_rmse: 0.0398
Epoch 16/100
1101/1101 [==============================] - 4s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0016 - val_rmse: 0.0391
Epoch 17/100
1101/1101 [==============================] - 4s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0018 - val_rmse: 0.0414
Epoch 18/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0365 - val_loss: 0.0017 - val_rmse: 0.0401
Epoch 19/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0361 - val_loss: 0.0017 - val_rmse: 0.0397
Epoch 20/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0362 - val_loss: 0.0016 - val_rmse: 0.0392
Epoch 21/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0366 - val_loss: 0.0017 - val_rmse: 0.0401
Epoch 22/100
1101/1101 [==============================] - 11s - loss: 0.0014 - rmse: 0.0367 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 23/100
1101/1101 [==============================] - 11s - loss: 0.0014 - rmse: 0.0372 - val_loss: 0.0016 - val_rmse: 0.0392
Epoch 24/100
1101/1101 [==============================] - 11s - loss: 0.0013 - rmse: 0.0356 - val_loss: 0.0017 - val_rmse: 0.0396
Epoch 25/100
1101/1101 [==============================] - 11s - loss: 0.0013 - rmse: 0.0358 - val_loss: 0.0016 - val_rmse: 0.0391
Epoch 26/100
1101/1101 [==============================] - 11s - loss: 0.0013 - rmse: 0.0360 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 27/100
1101/1101 [==============================] - 11s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0016 - val_rmse: 0.0392
Epoch 28/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0362 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 29/100
1101/1101 [==============================] - 10s - loss: 0.0012 - rmse: 0.0351 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 30/100
1101/1101 [==============================] - 9s - loss: 0.0014 - rmse: 0.0368 - val_loss: 0.0016 - val_rmse: 0.0392
Epoch 31/100
1101/1101 [==============================] - 11s - loss: 0.0012 - rmse: 0.0351 - val_loss: 0.0016 - val_rmse: 0.0390
Epoch 32/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0359 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 33/100
1101/1101 [==============================] - 10s - loss: 0.0012 - rmse: 0.0352 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 34/100
1101/1101 [==============================] - 11s - loss: 0.0013 - rmse: 0.0355 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 35/100
1101/1101 [==============================] - 10s - loss: 0.0013 - rmse: 0.0358 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 36/100
1101/1101 [==============================] - 11s - loss: 0.0012 - rmse: 0.0349 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 37/100
1101/1101 [==============================] - 11s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 38/100
1101/1101 [==============================] - 8s - loss: 0.0013 - rmse: 0.0356 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 39/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0344 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 40/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0345 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 41/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0342 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 42/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 43/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0341 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 44/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0341 - val_loss: 0.0017 - val_rmse: 0.0402
Epoch 45/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0344 - val_loss: 0.0017 - val_rmse: 0.0404
Epoch 46/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 47/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0342 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 48/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0344 - val_loss: 0.0018 - val_rmse: 0.0405
Epoch 49/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0339 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 50/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0338 - val_loss: 0.0018 - val_rmse: 0.0414
Epoch 51/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0347 - val_loss: 0.0016 - val_rmse: 0.0388
Epoch 52/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0348 - val_loss: 0.0017 - val_rmse: 0.0397
Epoch 53/100
1101/1101 [==============================] - 6s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0019 - val_rmse: 0.0421
Epoch 54/100
1101/1101 [==============================] - 6s - loss: 0.0013 - rmse: 0.0352 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 55/100
1101/1101 [==============================] - 6s - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 56/100
1101/1101 [==============================] - 6s - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0018 - val_rmse: 0.0406
Epoch 57/100
1101/1101 [==============================] - 5s - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 58/100
1101/1101 [==============================] - 6s - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0017 - val_rmse: 0.0391
Epoch 59/100
1101/1101 [==============================] - 6s - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 60/100
1101/1101 [==============================] - 5s - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0018 - val_rmse: 0.0412
Epoch 61/100
1101/1101 [==============================] - 5s - loss: 0.0012 - rmse: 0.0341 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 62/100
1101/1101 [==============================] - 4s - loss: 0.0011 - rmse: 0.0330 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 63/100
1101/1101 [==============================] - 4s - loss: 0.0011 - rmse: 0.0330 - val_loss: 0.0017 - val_rmse: 0.0395
Epoch 64/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0331 - val_loss: 0.0017 - val_rmse: 0.0397
Epoch 65/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 66/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 67/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0328 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 68/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0329 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 69/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 70/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 71/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0017 - val_rmse: 0.0393
Epoch 72/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0331 - val_loss: 0.0016 - val_rmse: 0.0388
Epoch 73/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0017 - val_rmse: 0.0393
Epoch 74/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 75/100
1101/1101 [==============================] - 3s - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 76/100
1101/1101 [==============================] - 3s - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 77/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 78/100
1101/1101 [==============================] - 3s - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 79/100
1101/1101 [==============================] - 3s - loss: 0.0010 - rmse: 0.0320 - val_loss: 0.0016 - val_rmse: 0.0378
Epoch 80/100
1101/1101 [==============================] - 3s - loss: 0.0010 - rmse: 0.0319 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 81/100
1101/1101 [==============================] - 3s - loss: 0.0011 - rmse: 0.0334 - val_loss: 0.0018 - val_rmse: 0.0404
Epoch 82/100
1101/1101 [==============================] - 3s - loss: 0.0010 - rmse: 0.0321 - val_loss: 0.0016 - val_rmse: 0.0379
Epoch 83/100
1101/1101 [==============================] - 2s - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 84/100
1101/1101 [==============================] - 2s - loss: 0.0010 - rmse: 0.0318 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 85/100
1101/1101 [==============================] - 2s - loss: 0.0010 - rmse: 0.0318 - val_loss: 0.0017 - val_rmse: 0.0396
Epoch 86/100
1101/1101 [==============================] - 2s - loss: 0.0010 - rmse: 0.0316 - val_loss: 0.0016 - val_rmse: 0.0379
Epoch 87/100
1101/1101 [==============================] - 2s - loss: 0.0010 - rmse: 0.0315 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 88/100
1101/1101 [==============================] - 2s - loss: 0.0011 - rmse: 0.0327 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 89/100
1101/1101 [==============================] - 2s - loss: 0.0010 - rmse: 0.0316 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 90/100
1101/1101 [==============================] - 3s - loss: 9.9906e-04 - rmse: 0.0315 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 91/100
1101/1101 [==============================] - 7s - loss: 9.9414e-04 - rmse: 0.0314 - val_loss: 0.0017 - val_rmse: 0.0395
Epoch 92/100
1101/1101 [==============================] - 7s - loss: 9.7411e-04 - rmse: 0.0311 - val_loss: 0.0017 - val_rmse: 0.0393
Epoch 93/100
1101/1101 [==============================] - 7s - loss: 9.9650e-04 - rmse: 0.0315 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 94/100
1101/1101 [==============================] - 7s - loss: 9.9459e-04 - rmse: 0.0315 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 95/100
1101/1101 [==============================] - 8s - loss: 9.7369e-04 - rmse: 0.0311 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 96/100
1101/1101 [==============================] - 7s - loss: 9.7156e-04 - rmse: 0.0311 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 97/100
1101/1101 [==============================] - 7s - loss: 9.8241e-04 - rmse: 0.0313 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 98/100
1101/1101 [==============================] - 7s - loss: 9.6614e-04 - rmse: 0.0310 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 99/100
1101/1101 [==============================] - 7s - loss: 0.0010 - rmse: 0.0321 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 100/100
1101/1101 [==============================] - 7s - loss: 9.4840e-04 - rmse: 0.0307 - val_loss: 0.0016 - val_rmse: 0.0379
==========
evaluating using the final model
Test score: 0.001582 rmse (norm): 0.039773 rmse (real): 5.210310