Using TensorFlow backend.
2018-01-09 03:12:02.511338: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 03:12:02.511413: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 03:12:02.511437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 03:12:02.511453: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 03:12:02.511470: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-09 03:12:03.738766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:06:00.0
Total memory: 11.92GiB
Free memory: 3.18GiB
2018-01-09 03:12:03.739049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-01-09 03:12:03.739093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-01-09 03:12:03.739187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)
loading data...
train_data shape:  (1104, 1, 16, 16)
min: 0.0 max: 262.0
XC shape:  (1248, 3, 16, 16) XP shape:  (1248, 4, 16, 16) Y shape: (1248, 1, 16, 16)
XC shape:  (1248, 3, 16, 16) XP shape:  (1248, 4, 16, 16) Y shape: (1248, 1, 16, 16)
2 2
==========
compiling model...
/home/quyuan/GP/code/mydeepst/models/STResNet.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding="same", filters=64, kernel_size=(3, 3))`
  nb_filter=64, nb_row=3, nb_col=3, border_mode="same")(input)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding="same", strides=(1, 1), filters=64, kernel_size=(3, 3))`
  border_mode="same")(activation)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:22: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  return merge([input, residual], mode='sum')
/usr/lib64/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  name=name)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding="same", filters=1, kernel_size=(3, 3))`
  nb_filter=1, nb_row=3, nb_col=3, border_mode="same")(activation)
/usr/lib64/python2.7/site-packages/keras/engine/topology.py:621: UserWarning: Class `mydeepst.models.iLayer.iLayer` defines `get_output_shape_for` but does not override `compute_output_shape`. If this is a Keras 1 layer, please implement `compute_output_shape` to support Keras 2.
  output_shape = self.compute_output_shape(input_shape)
/home/quyuan/GP/code/mydeepst/models/STResNet.py:97: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  main_output = merge(new_outputs, mode='sum')
/home/quyuan/GP/code/mydeepst/models/STResNet.py:114: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("ac..., inputs=[<tf.Tenso...)`
  model = Model(input=main_inputs, output=main_output)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 3, 16, 16)     0                                            
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 4, 16, 16)     0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 64, 16, 16)    1792        input_1[0][0]                    
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 64, 16, 16)    2368        input_2[0][0]                    
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 64, 16, 16)    0           conv2d_1[0][0]                   
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 64, 16, 16)    0           conv2d_11[0][0]                  
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 64, 16, 16)    36928       activation_1[0][0]               
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 64, 16, 16)    36928       activation_10[0][0]              
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 64, 16, 16)    0           conv2d_2[0][0]                   
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 64, 16, 16)    0           conv2d_12[0][0]                  
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 64, 16, 16)    36928       activation_2[0][0]               
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 64, 16, 16)    36928       activation_11[0][0]              
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 64, 16, 16)    0           conv2d_1[0][0]                   
                                                                   conv2d_3[0][0]                   
____________________________________________________________________________________________________
merge_5 (Merge)                  (None, 64, 16, 16)    0           conv2d_11[0][0]                  
                                                                   conv2d_13[0][0]                  
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 64, 16, 16)    0           merge_1[0][0]                    
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 64, 16, 16)    0           merge_5[0][0]                    
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 64, 16, 16)    36928       activation_3[0][0]               
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 64, 16, 16)    36928       activation_12[0][0]              
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 64, 16, 16)    0           conv2d_4[0][0]                   
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 64, 16, 16)    0           conv2d_14[0][0]                  
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 64, 16, 16)    36928       activation_4[0][0]               
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 64, 16, 16)    36928       activation_13[0][0]              
____________________________________________________________________________________________________
merge_2 (Merge)                  (None, 64, 16, 16)    0           merge_1[0][0]                    
                                                                   conv2d_5[0][0]                   
____________________________________________________________________________________________________
merge_6 (Merge)                  (None, 64, 16, 16)    0           merge_5[0][0]                    
                                                                   conv2d_15[0][0]                  
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 64, 16, 16)    0           merge_2[0][0]                    
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 64, 16, 16)    0           merge_6[0][0]                    
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 64, 16, 16)    36928       activation_5[0][0]               
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 64, 16, 16)    36928       activation_14[0][0]              
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 64, 16, 16)    0           conv2d_6[0][0]                   
____________________________________________________________________________________________________
activation_15 (Activation)       (None, 64, 16, 16)    0           conv2d_16[0][0]                  
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 64, 16, 16)    36928       activation_6[0][0]               
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 64, 16, 16)    36928       activation_15[0][0]              
____________________________________________________________________________________________________
merge_3 (Merge)                  (None, 64, 16, 16)    0           merge_2[0][0]                    
                                                                   conv2d_7[0][0]                   
____________________________________________________________________________________________________
merge_7 (Merge)                  (None, 64, 16, 16)    0           merge_6[0][0]                    
                                                                   conv2d_17[0][0]                  
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 64, 16, 16)    0           merge_3[0][0]                    
____________________________________________________________________________________________________
activation_16 (Activation)       (None, 64, 16, 16)    0           merge_7[0][0]                    
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 64, 16, 16)    36928       activation_7[0][0]               
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 64, 16, 16)    36928       activation_16[0][0]              
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 64, 16, 16)    0           conv2d_8[0][0]                   
____________________________________________________________________________________________________
activation_17 (Activation)       (None, 64, 16, 16)    0           conv2d_18[0][0]                  
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 64, 16, 16)    36928       activation_8[0][0]               
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 64, 16, 16)    36928       activation_17[0][0]              
____________________________________________________________________________________________________
merge_4 (Merge)                  (None, 64, 16, 16)    0           merge_3[0][0]                    
                                                                   conv2d_9[0][0]                   
____________________________________________________________________________________________________
merge_8 (Merge)                  (None, 64, 16, 16)    0           merge_7[0][0]                    
                                                                   conv2d_19[0][0]                  
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 64, 16, 16)    0           merge_4[0][0]                    
____________________________________________________________________________________________________
activation_18 (Activation)       (None, 64, 16, 16)    0           merge_8[0][0]                    
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 1, 16, 16)     577         activation_9[0][0]               
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 1, 16, 16)     577         activation_18[0][0]              
____________________________________________________________________________________________________
i_layer_1 (iLayer)               (None, 1, 16, 16)     256         conv2d_10[0][0]                  
____________________________________________________________________________________________________
i_layer_2 (iLayer)               (None, 1, 16, 16)     256         conv2d_20[0][0]                  
____________________________________________________________________________________________________
merge_9 (Merge)                  (None, 1, 16, 16)     0           i_layer_1[0][0]                  
                                                                   i_layer_2[0][0]                  
____________________________________________________________________________________________________
activation_19 (Activation)       (None, 1, 16, 16)     0           merge_9[0][0]                    
====================================================================================================
Total params: 596,674
Trainable params: 596,674
Non-trainable params: 0
____________________________________________________________________________________________________
==========
training model...
main.py:83: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  verbose=1)
Train on 820 samples, validate on 92 samples
Epoch 1/500
2018-01-09 03:12:20.841106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0)
820/820 [==============================] - 7s - loss: 0.1121 - rmse: 0.2383 - val_loss: 0.0222 - val_rmse: 0.1438
Epoch 2/500
820/820 [==============================] - 4s - loss: 0.0215 - rmse: 0.1459 - val_loss: 0.0219 - val_rmse: 0.1425
Epoch 3/500
820/820 [==============================] - 4s - loss: 0.0214 - rmse: 0.1458 - val_loss: 0.0218 - val_rmse: 0.1424
Epoch 4/500
820/820 [==============================] - 4s - loss: 0.0214 - rmse: 0.1457 - val_loss: 0.0218 - val_rmse: 0.1424
Epoch 5/500
820/820 [==============================] - 4s - loss: 0.0214 - rmse: 0.1449 - val_loss: 0.0217 - val_rmse: 0.1421
Epoch 6/500
820/820 [==============================] - 4s - loss: 0.0212 - rmse: 0.1445 - val_loss: 0.0212 - val_rmse: 0.1407
Epoch 7/500
820/820 [==============================] - 4s - loss: 0.0206 - rmse: 0.1426 - val_loss: 0.0202 - val_rmse: 0.1373
Epoch 8/500
820/820 [==============================] - 3s - loss: 0.0198 - rmse: 0.1396 - val_loss: 0.0198 - val_rmse: 0.1359
Epoch 9/500
820/820 [==============================] - 4s - loss: 0.0194 - rmse: 0.1386 - val_loss: 0.0189 - val_rmse: 0.1334
Epoch 10/500
820/820 [==============================] - 4s - loss: 0.0182 - rmse: 0.1341 - val_loss: 0.0175 - val_rmse: 0.1287
Epoch 11/500
820/820 [==============================] - 4s - loss: 0.0161 - rmse: 0.1262 - val_loss: 0.0153 - val_rmse: 0.1203
Epoch 12/500
820/820 [==============================] - 4s - loss: 0.0147 - rmse: 0.1208 - val_loss: 0.0155 - val_rmse: 0.1214
Epoch 13/500
820/820 [==============================] - 4s - loss: 0.0146 - rmse: 0.1202 - val_loss: 0.0150 - val_rmse: 0.1195
Epoch 14/500
820/820 [==============================] - 4s - loss: 0.0123 - rmse: 0.1101 - val_loss: 0.0117 - val_rmse: 0.1065
Epoch 15/500
820/820 [==============================] - 4s - loss: 0.0102 - rmse: 0.1006 - val_loss: 0.0100 - val_rmse: 0.0987
Epoch 16/500
820/820 [==============================] - 4s - loss: 0.0095 - rmse: 0.0967 - val_loss: 0.0097 - val_rmse: 0.0968
Epoch 17/500
820/820 [==============================] - 4s - loss: 0.0091 - rmse: 0.0951 - val_loss: 0.0095 - val_rmse: 0.0964
Epoch 18/500
820/820 [==============================] - 4s - loss: 0.0086 - rmse: 0.0923 - val_loss: 0.0074 - val_rmse: 0.0858
Epoch 19/500
820/820 [==============================] - 4s - loss: 0.0067 - rmse: 0.0812 - val_loss: 0.0062 - val_rmse: 0.0780
Epoch 20/500
820/820 [==============================] - 4s - loss: 0.0056 - rmse: 0.0746 - val_loss: 0.0053 - val_rmse: 0.0721
Epoch 21/500
820/820 [==============================] - 4s - loss: 0.0051 - rmse: 0.0712 - val_loss: 0.0048 - val_rmse: 0.0692
Epoch 22/500
820/820 [==============================] - 4s - loss: 0.0045 - rmse: 0.0664 - val_loss: 0.0042 - val_rmse: 0.0642
Epoch 23/500
820/820 [==============================] - 4s - loss: 0.0037 - rmse: 0.0607 - val_loss: 0.0036 - val_rmse: 0.0600
Epoch 24/500
820/820 [==============================] - 4s - loss: 0.0034 - rmse: 0.0581 - val_loss: 0.0038 - val_rmse: 0.0614
Epoch 25/500
820/820 [==============================] - 4s - loss: 0.0032 - rmse: 0.0567 - val_loss: 0.0033 - val_rmse: 0.0570
Epoch 26/500
820/820 [==============================] - 4s - loss: 0.0030 - rmse: 0.0549 - val_loss: 0.0030 - val_rmse: 0.0550
Epoch 27/500
820/820 [==============================] - 4s - loss: 0.0029 - rmse: 0.0541 - val_loss: 0.0030 - val_rmse: 0.0543
Epoch 28/500
820/820 [==============================] - 4s - loss: 0.0028 - rmse: 0.0530 - val_loss: 0.0031 - val_rmse: 0.0552
Epoch 29/500
820/820 [==============================] - 4s - loss: 0.0028 - rmse: 0.0525 - val_loss: 0.0028 - val_rmse: 0.0530
Epoch 30/500
820/820 [==============================] - 4s - loss: 0.0027 - rmse: 0.0513 - val_loss: 0.0027 - val_rmse: 0.0522
Epoch 31/500
820/820 [==============================] - 3s - loss: 0.0027 - rmse: 0.0517 - val_loss: 0.0029 - val_rmse: 0.0534
Epoch 32/500
820/820 [==============================] - 2s - loss: 0.0026 - rmse: 0.0508 - val_loss: 0.0027 - val_rmse: 0.0515
Epoch 33/500
820/820 [==============================] - 2s - loss: 0.0026 - rmse: 0.0504 - val_loss: 0.0028 - val_rmse: 0.0524
Epoch 34/500
820/820 [==============================] - 2s - loss: 0.0025 - rmse: 0.0498 - val_loss: 0.0029 - val_rmse: 0.0537
Epoch 35/500
820/820 [==============================] - 2s - loss: 0.0026 - rmse: 0.0505 - val_loss: 0.0027 - val_rmse: 0.0521
Epoch 36/500
820/820 [==============================] - 2s - loss: 0.0025 - rmse: 0.0496 - val_loss: 0.0027 - val_rmse: 0.0513
Epoch 37/500
820/820 [==============================] - 4s - loss: 0.0024 - rmse: 0.0491 - val_loss: 0.0025 - val_rmse: 0.0500
Epoch 38/500
820/820 [==============================] - 9s - loss: 0.0024 - rmse: 0.0491 - val_loss: 0.0026 - val_rmse: 0.0504
Epoch 39/500
820/820 [==============================] - 8s - loss: 0.0024 - rmse: 0.0490 - val_loss: 0.0025 - val_rmse: 0.0503
Epoch 40/500
820/820 [==============================] - 7s - loss: 0.0025 - rmse: 0.0495 - val_loss: 0.0027 - val_rmse: 0.0516
Epoch 41/500
820/820 [==============================] - 8s - loss: 0.0024 - rmse: 0.0486 - val_loss: 0.0025 - val_rmse: 0.0497
Epoch 42/500
820/820 [==============================] - 7s - loss: 0.0024 - rmse: 0.0487 - val_loss: 0.0026 - val_rmse: 0.0504
Epoch 43/500
820/820 [==============================] - 8s - loss: 0.0024 - rmse: 0.0487 - val_loss: 0.0024 - val_rmse: 0.0491
Epoch 44/500
820/820 [==============================] - 8s - loss: 0.0023 - rmse: 0.0477 - val_loss: 0.0025 - val_rmse: 0.0495
Epoch 45/500
820/820 [==============================] - 7s - loss: 0.0023 - rmse: 0.0474 - val_loss: 0.0024 - val_rmse: 0.0483
Epoch 46/500
820/820 [==============================] - 8s - loss: 0.0022 - rmse: 0.0472 - val_loss: 0.0023 - val_rmse: 0.0480
Epoch 47/500
820/820 [==============================] - 8s - loss: 0.0022 - rmse: 0.0464 - val_loss: 0.0023 - val_rmse: 0.0479
Epoch 48/500
820/820 [==============================] - 8s - loss: 0.0022 - rmse: 0.0464 - val_loss: 0.0023 - val_rmse: 0.0476
Epoch 49/500
820/820 [==============================] - 7s - loss: 0.0022 - rmse: 0.0463 - val_loss: 0.0023 - val_rmse: 0.0480
Epoch 50/500
820/820 [==============================] - 7s - loss: 0.0021 - rmse: 0.0461 - val_loss: 0.0023 - val_rmse: 0.0477
Epoch 51/500
820/820 [==============================] - 7s - loss: 0.0021 - rmse: 0.0458 - val_loss: 0.0023 - val_rmse: 0.0478
Epoch 52/500
820/820 [==============================] - 8s - loss: 0.0020 - rmse: 0.0451 - val_loss: 0.0022 - val_rmse: 0.0465
Epoch 53/500
820/820 [==============================] - 7s - loss: 0.0020 - rmse: 0.0450 - val_loss: 0.0022 - val_rmse: 0.0466
Epoch 54/500
820/820 [==============================] - 7s - loss: 0.0020 - rmse: 0.0450 - val_loss: 0.0022 - val_rmse: 0.0466
Epoch 55/500
820/820 [==============================] - 8s - loss: 0.0021 - rmse: 0.0456 - val_loss: 0.0021 - val_rmse: 0.0459
Epoch 56/500
820/820 [==============================] - 7s - loss: 0.0020 - rmse: 0.0444 - val_loss: 0.0022 - val_rmse: 0.0469
Epoch 57/500
820/820 [==============================] - 8s - loss: 0.0020 - rmse: 0.0441 - val_loss: 0.0021 - val_rmse: 0.0453
Epoch 58/500
820/820 [==============================] - 8s - loss: 0.0019 - rmse: 0.0434 - val_loss: 0.0020 - val_rmse: 0.0447
Epoch 59/500
820/820 [==============================] - 8s - loss: 0.0019 - rmse: 0.0432 - val_loss: 0.0020 - val_rmse: 0.0447
Epoch 60/500
820/820 [==============================] - 8s - loss: 0.0019 - rmse: 0.0430 - val_loss: 0.0020 - val_rmse: 0.0445
Epoch 61/500
820/820 [==============================] - 8s - loss: 0.0019 - rmse: 0.0431 - val_loss: 0.0020 - val_rmse: 0.0449
Epoch 62/500
820/820 [==============================] - 4s - loss: 0.0019 - rmse: 0.0431 - val_loss: 0.0023 - val_rmse: 0.0475
Epoch 63/500
820/820 [==============================] - 4s - loss: 0.0019 - rmse: 0.0432 - val_loss: 0.0023 - val_rmse: 0.0479
Epoch 64/500
820/820 [==============================] - 4s - loss: 0.0019 - rmse: 0.0431 - val_loss: 0.0020 - val_rmse: 0.0439
Epoch 65/500
820/820 [==============================] - 4s - loss: 0.0019 - rmse: 0.0434 - val_loss: 0.0024 - val_rmse: 0.0487
Epoch 66/500
820/820 [==============================] - 3s - loss: 0.0018 - rmse: 0.0427 - val_loss: 0.0020 - val_rmse: 0.0442
Epoch 67/500
820/820 [==============================] - 3s - loss: 0.0018 - rmse: 0.0426 - val_loss: 0.0020 - val_rmse: 0.0445
Epoch 68/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0422 - val_loss: 0.0020 - val_rmse: 0.0439
Epoch 69/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0423 - val_loss: 0.0020 - val_rmse: 0.0444
Epoch 70/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0420 - val_loss: 0.0020 - val_rmse: 0.0444
Epoch 71/500
820/820 [==============================] - 3s - loss: 0.0018 - rmse: 0.0419 - val_loss: 0.0020 - val_rmse: 0.0439
Epoch 72/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0417 - val_loss: 0.0019 - val_rmse: 0.0438
Epoch 73/500
820/820 [==============================] - 4s - loss: 0.0017 - rmse: 0.0416 - val_loss: 0.0020 - val_rmse: 0.0448
Epoch 74/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0425 - val_loss: 0.0022 - val_rmse: 0.0468
Epoch 75/500
820/820 [==============================] - 4s - loss: 0.0017 - rmse: 0.0413 - val_loss: 0.0019 - val_rmse: 0.0430
Epoch 76/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0427 - val_loss: 0.0020 - val_rmse: 0.0447
Epoch 77/500
820/820 [==============================] - 4s - loss: 0.0017 - rmse: 0.0412 - val_loss: 0.0020 - val_rmse: 0.0449
Epoch 78/500
820/820 [==============================] - 4s - loss: 0.0018 - rmse: 0.0418 - val_loss: 0.0019 - val_rmse: 0.0432
Epoch 79/500
820/820 [==============================] - 4s - loss: 0.0017 - rmse: 0.0414 - val_loss: 0.0019 - val_rmse: 0.0434
Epoch 80/500
820/820 [==============================] - 4s - loss: 0.0017 - rmse: 0.0405 - val_loss: 0.0018 - val_rmse: 0.0421
Epoch 81/500
820/820 [==============================] - 4s - loss: 0.0016 - rmse: 0.0404 - val_loss: 0.0017 - val_rmse: 0.0415
Epoch 82/500
820/820 [==============================] - 4s - loss: 0.0016 - rmse: 0.0403 - val_loss: 0.0019 - val_rmse: 0.0427
Epoch 83/500
820/820 [==============================] - 4s - loss: 0.0016 - rmse: 0.0401 - val_loss: 0.0018 - val_rmse: 0.0417
Epoch 84/500
820/820 [==============================] - 4s - loss: 0.0016 - rmse: 0.0398 - val_loss: 0.0019 - val_rmse: 0.0428
Epoch 85/500
820/820 [==============================] - 3s - loss: 0.0016 - rmse: 0.0402 - val_loss: 0.0020 - val_rmse: 0.0448
Epoch 86/500
820/820 [==============================] - 4s - loss: 0.0016 - rmse: 0.0398 - val_loss: 0.0017 - val_rmse: 0.0411
Epoch 87/500
820/820 [==============================] - 3s - loss: 0.0015 - rmse: 0.0391 - val_loss: 0.0018 - val_rmse: 0.0423
Epoch 88/500
820/820 [==============================] - 4s - loss: 0.0015 - rmse: 0.0391 - val_loss: 0.0017 - val_rmse: 0.0407
Epoch 89/500
820/820 [==============================] - 4s - loss: 0.0017 - rmse: 0.0407 - val_loss: 0.0018 - val_rmse: 0.0426
Epoch 90/500
820/820 [==============================] - 3s - loss: 0.0015 - rmse: 0.0389 - val_loss: 0.0017 - val_rmse: 0.0406
Epoch 91/500
820/820 [==============================] - 2s - loss: 0.0015 - rmse: 0.0381 - val_loss: 0.0017 - val_rmse: 0.0410
Epoch 92/500
820/820 [==============================] - 2s - loss: 0.0015 - rmse: 0.0382 - val_loss: 0.0018 - val_rmse: 0.0416
Epoch 93/500
820/820 [==============================] - 2s - loss: 0.0015 - rmse: 0.0381 - val_loss: 0.0017 - val_rmse: 0.0407
Epoch 94/500
820/820 [==============================] - 2s - loss: 0.0015 - rmse: 0.0382 - val_loss: 0.0017 - val_rmse: 0.0405
Epoch 95/500
820/820 [==============================] - 2s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0017 - val_rmse: 0.0407
Epoch 96/500
820/820 [==============================] - 2s - loss: 0.0015 - rmse: 0.0382 - val_loss: 0.0019 - val_rmse: 0.0433
Epoch 97/500
820/820 [==============================] - 3s - loss: 0.0014 - rmse: 0.0374 - val_loss: 0.0016 - val_rmse: 0.0395
Epoch 98/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0373 - val_loss: 0.0016 - val_rmse: 0.0402
Epoch 99/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0376 - val_loss: 0.0018 - val_rmse: 0.0424
Epoch 100/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0369 - val_loss: 0.0017 - val_rmse: 0.0403
Epoch 101/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0369 - val_loss: 0.0016 - val_rmse: 0.0402
Epoch 102/500
820/820 [==============================] - 7s - loss: 0.0014 - rmse: 0.0378 - val_loss: 0.0017 - val_rmse: 0.0402
Epoch 103/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0366 - val_loss: 0.0016 - val_rmse: 0.0394
Epoch 104/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0369 - val_loss: 0.0018 - val_rmse: 0.0421
Epoch 105/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0367 - val_loss: 0.0016 - val_rmse: 0.0400
Epoch 106/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0368 - val_loss: 0.0016 - val_rmse: 0.0396
Epoch 107/500
820/820 [==============================] - 8s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0016 - val_rmse: 0.0394
Epoch 108/500
820/820 [==============================] - 7s - loss: 0.0014 - rmse: 0.0373 - val_loss: 0.0016 - val_rmse: 0.0401
Epoch 109/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0366 - val_loss: 0.0016 - val_rmse: 0.0395
Epoch 110/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0368 - val_loss: 0.0021 - val_rmse: 0.0454
Epoch 111/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0370 - val_loss: 0.0017 - val_rmse: 0.0410
Epoch 112/500
820/820 [==============================] - 8s - loss: 0.0013 - rmse: 0.0365 - val_loss: 0.0016 - val_rmse: 0.0397
Epoch 113/500
820/820 [==============================] - 8s - loss: 0.0014 - rmse: 0.0370 - val_loss: 0.0023 - val_rmse: 0.0471
==========
evaluating using the model that has the best loss on the valid set
Train score: 0.002000 rmse (norm): 0.043707 rmse (real): 5.725654
Test score: 0.002414 rmse (norm): 0.049129 rmse (real): 6.435869
==========
training model (cont)...
main.py:105: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  model_checkpoint], validation_data=(X_test, Y_test))
Train on 912 samples, validate on 336 samples
Epoch 1/100
912/912 [==============================] - 10s - loss: 0.0015 - rmse: 0.0380 - val_loss: 0.0017 - val_rmse: 0.0395
Epoch 2/100
912/912 [==============================] - 9s - loss: 0.0013 - rmse: 0.0362 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 3/100
912/912 [==============================] - 10s - loss: 0.0013 - rmse: 0.0360 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 4/100
912/912 [==============================] - 9s - loss: 0.0013 - rmse: 0.0358 - val_loss: 0.0017 - val_rmse: 0.0396
Epoch 5/100
912/912 [==============================] - 9s - loss: 0.0015 - rmse: 0.0379 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 6/100
912/912 [==============================] - 9s - loss: 0.0013 - rmse: 0.0361 - val_loss: 0.0018 - val_rmse: 0.0407
Epoch 7/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0364 - val_loss: 0.0018 - val_rmse: 0.0410
Epoch 8/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0361 - val_loss: 0.0017 - val_rmse: 0.0397
Epoch 9/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0360 - val_loss: 0.0017 - val_rmse: 0.0392
Epoch 10/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0359 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 11/100
912/912 [==============================] - 5s - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0017 - val_rmse: 0.0393
Epoch 12/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 13/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0352 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 14/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0353 - val_loss: 0.0016 - val_rmse: 0.0390
Epoch 15/100
912/912 [==============================] - 5s - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0016 - val_rmse: 0.0388
Epoch 16/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0352 - val_loss: 0.0017 - val_rmse: 0.0396
Epoch 17/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0358 - val_loss: 0.0017 - val_rmse: 0.0398
Epoch 18/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 19/100
912/912 [==============================] - 5s - loss: 0.0012 - rmse: 0.0347 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 20/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0016 - val_rmse: 0.0388
Epoch 21/100
912/912 [==============================] - 5s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 22/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0357 - val_loss: 0.0017 - val_rmse: 0.0401
Epoch 23/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0350 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 24/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0017 - val_rmse: 0.0400
Epoch 25/100
912/912 [==============================] - 4s - loss: 0.0013 - rmse: 0.0353 - val_loss: 0.0017 - val_rmse: 0.0399
Epoch 26/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0351 - val_loss: 0.0017 - val_rmse: 0.0402
Epoch 27/100
912/912 [==============================] - 5s - loss: 0.0012 - rmse: 0.0344 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 28/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0344 - val_loss: 0.0017 - val_rmse: 0.0393
Epoch 29/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0346 - val_loss: 0.0016 - val_rmse: 0.0388
Epoch 30/100
912/912 [==============================] - 5s - loss: 0.0012 - rmse: 0.0342 - val_loss: 0.0018 - val_rmse: 0.0404
Epoch 31/100
912/912 [==============================] - 4s - loss: 0.0012 - rmse: 0.0341 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 32/100
912/912 [==============================] - 3s - loss: 0.0011 - rmse: 0.0338 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 33/100
912/912 [==============================] - 3s - loss: 0.0012 - rmse: 0.0343 - val_loss: 0.0018 - val_rmse: 0.0407
Epoch 34/100
912/912 [==============================] - 2s - loss: 0.0012 - rmse: 0.0340 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 35/100
912/912 [==============================] - 3s - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 36/100
912/912 [==============================] - 3s - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 37/100
912/912 [==============================] - 3s - loss: 0.0012 - rmse: 0.0339 - val_loss: 0.0017 - val_rmse: 0.0401
Epoch 38/100
912/912 [==============================] - 9s - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0018 - val_rmse: 0.0408
Epoch 39/100
912/912 [==============================] - 14s - loss: 0.0012 - rmse: 0.0345 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 40/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0017 - val_rmse: 0.0394
Epoch 41/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 42/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0337 - val_loss: 0.0017 - val_rmse: 0.0392
Epoch 43/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0332 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 44/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0336 - val_loss: 0.0017 - val_rmse: 0.0395
Epoch 45/100
912/912 [==============================] - 15s - loss: 0.0011 - rmse: 0.0331 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 46/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0333 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 47/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0331 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 48/100
912/912 [==============================] - 15s - loss: 0.0011 - rmse: 0.0327 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 49/100
912/912 [==============================] - 15s - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0017 - val_rmse: 0.0396
Epoch 50/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0328 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 51/100
912/912 [==============================] - 15s - loss: 0.0011 - rmse: 0.0328 - val_loss: 0.0016 - val_rmse: 0.0387
Epoch 52/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0330 - val_loss: 0.0016 - val_rmse: 0.0386
Epoch 53/100
912/912 [==============================] - 14s - loss: 0.0011 - rmse: 0.0330 - val_loss: 0.0016 - val_rmse: 0.0388
Epoch 54/100
912/912 [==============================] - 11s - loss: 0.0011 - rmse: 0.0325 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 55/100
912/912 [==============================] - 9s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 56/100
912/912 [==============================] - 9s - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 57/100
912/912 [==============================] - 8s - loss: 0.0010 - rmse: 0.0322 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 58/100
912/912 [==============================] - 8s - loss: 0.0010 - rmse: 0.0321 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 59/100
912/912 [==============================] - 8s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0017 - val_rmse: 0.0391
Epoch 60/100
912/912 [==============================] - 8s - loss: 0.0010 - rmse: 0.0318 - val_loss: 0.0017 - val_rmse: 0.0396
Epoch 61/100
912/912 [==============================] - 6s - loss: 0.0010 - rmse: 0.0320 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 62/100
912/912 [==============================] - 4s - loss: 0.0011 - rmse: 0.0324 - val_loss: 0.0018 - val_rmse: 0.0402
Epoch 63/100
912/912 [==============================] - 4s - loss: 0.0010 - rmse: 0.0321 - val_loss: 0.0016 - val_rmse: 0.0389
Epoch 64/100
912/912 [==============================] - 5s - loss: 0.0010 - rmse: 0.0316 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 65/100
912/912 [==============================] - 5s - loss: 9.6728e-04 - rmse: 0.0310 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 66/100
912/912 [==============================] - 4s - loss: 9.7330e-04 - rmse: 0.0311 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 67/100
912/912 [==============================] - 4s - loss: 9.7184e-04 - rmse: 0.0311 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 68/100
912/912 [==============================] - 4s - loss: 9.9660e-04 - rmse: 0.0315 - val_loss: 0.0017 - val_rmse: 0.0391
Epoch 69/100
912/912 [==============================] - 4s - loss: 0.0010 - rmse: 0.0316 - val_loss: 0.0016 - val_rmse: 0.0379
Epoch 70/100
912/912 [==============================] - 4s - loss: 9.4672e-04 - rmse: 0.0307 - val_loss: 0.0016 - val_rmse: 0.0384
Epoch 71/100
912/912 [==============================] - 5s - loss: 9.3803e-04 - rmse: 0.0305 - val_loss: 0.0017 - val_rmse: 0.0390
Epoch 72/100
912/912 [==============================] - 4s - loss: 9.2876e-04 - rmse: 0.0304 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 73/100
912/912 [==============================] - 4s - loss: 9.4252e-04 - rmse: 0.0306 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 74/100
912/912 [==============================] - 3s - loss: 9.8688e-04 - rmse: 0.0313 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 75/100
912/912 [==============================] - 2s - loss: 9.3411e-04 - rmse: 0.0305 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 76/100
912/912 [==============================] - 3s - loss: 9.3711e-04 - rmse: 0.0305 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 77/100
912/912 [==============================] - 2s - loss: 9.8869e-04 - rmse: 0.0313 - val_loss: 0.0017 - val_rmse: 0.0399
Epoch 78/100
912/912 [==============================] - 3s - loss: 9.5946e-04 - rmse: 0.0309 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 79/100
912/912 [==============================] - 2s - loss: 9.6189e-04 - rmse: 0.0309 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 80/100
912/912 [==============================] - 9s - loss: 9.1361e-04 - rmse: 0.0301 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 81/100
912/912 [==============================] - 9s - loss: 9.2667e-04 - rmse: 0.0303 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 82/100
912/912 [==============================] - 9s - loss: 8.9853e-04 - rmse: 0.0299 - val_loss: 0.0016 - val_rmse: 0.0382
Epoch 83/100
912/912 [==============================] - 9s - loss: 9.0369e-04 - rmse: 0.0300 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 84/100
912/912 [==============================] - 9s - loss: 8.8082e-04 - rmse: 0.0296 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 85/100
912/912 [==============================] - 9s - loss: 8.7436e-04 - rmse: 0.0295 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 86/100
912/912 [==============================] - 9s - loss: 8.8337e-04 - rmse: 0.0297 - val_loss: 0.0016 - val_rmse: 0.0383
Epoch 87/100
912/912 [==============================] - 9s - loss: 8.9116e-04 - rmse: 0.0297 - val_loss: 0.0016 - val_rmse: 0.0377
Epoch 88/100
912/912 [==============================] - 9s - loss: 8.4045e-04 - rmse: 0.0289 - val_loss: 0.0016 - val_rmse: 0.0379
Epoch 89/100
912/912 [==============================] - 9s - loss: 8.4350e-04 - rmse: 0.0290 - val_loss: 0.0016 - val_rmse: 0.0378
Epoch 90/100
912/912 [==============================] - 10s - loss: 8.3419e-04 - rmse: 0.0288 - val_loss: 0.0016 - val_rmse: 0.0377
Epoch 91/100
912/912 [==============================] - 9s - loss: 8.4169e-04 - rmse: 0.0289 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 92/100
912/912 [==============================] - 9s - loss: 8.4289e-04 - rmse: 0.0290 - val_loss: 0.0016 - val_rmse: 0.0385
Epoch 93/100
912/912 [==============================] - 9s - loss: 8.3658e-04 - rmse: 0.0289 - val_loss: 0.0017 - val_rmse: 0.0389
Epoch 94/100
912/912 [==============================] - 9s - loss: 8.2984e-04 - rmse: 0.0287 - val_loss: 0.0016 - val_rmse: 0.0380
Epoch 95/100
912/912 [==============================] - 9s - loss: 8.4209e-04 - rmse: 0.0289 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 96/100
912/912 [==============================] - 9s - loss: 7.9967e-04 - rmse: 0.0282 - val_loss: 0.0016 - val_rmse: 0.0379
Epoch 97/100
912/912 [==============================] - 8s - loss: 8.2777e-04 - rmse: 0.0287 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 98/100
912/912 [==============================] - 8s - loss: 8.2275e-04 - rmse: 0.0286 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 99/100
912/912 [==============================] - 9s - loss: 8.2173e-04 - rmse: 0.0286 - val_loss: 0.0016 - val_rmse: 0.0381
Epoch 100/100
912/912 [==============================] - 8s - loss: 8.4777e-04 - rmse: 0.0290 - val_loss: 0.0016 - val_rmse: 0.0385
==========
evaluating using the final model
Test score: 0.001619 rmse (norm): 0.040239 rmse (real): 5.271366
5.176784